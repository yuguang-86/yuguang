<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Loan Prediction Data Preparation</title>
<style>
/* Optional: Basic styling for readability */
body {
    font-family: Arial, sans-serif;
    line-height: 1.6;
    margin: 20px;
    max-width: 1000px;
}
h1, h2 {
    color: #333;
}
pre {
    background: #f5f5f5;
    border: 1px solid #ccc;
    padding: 10px;
    overflow: auto;
}
code {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
    font-size: 14px;
}
</style>
</head>
<body>

<h1>Loan Prediction Data Preparation</h1>

<p>This document shows the code used for cleaning and standardizing loan datasets. The first script removes unwanted columns, handles missing values, and prepares the <code>loans_to_predict.csv</code> dataset for modeling. The second script standardizes the numeric features and transforms date columns into numeric formats. Both scripts ensure that the data fed into the predictive model is consistent, clean, and ready for analysis.</p>

<hr>

<h2>Data Cleaning (for <code>loans_to_predict.csv</code>)</h2>
<p>
This code snippet reads the raw prediction dataset, removes specified columns known to be uninformative or problematic (e.g., too many missing values, unnecessary identifiers, or single-value columns), and then outputs the cleaned dataset to a new file <code>loans_to_predict_cleaned.csv</code>. It also prints out information on remaining columns, data types, and missing values after cleaning.
</p>

<pre><code class="language-python">
import pandas as pd
import numpy as np

# Read the data
test_data = pd.read_csv('/Users/yuguangsong/Documents/Datasets/loans_to_predict.csv')

# Drop specified columns
columns_to_drop = [
    'desc',                      # High percentage of missing values
    'mths_since_last_record',    # High percentage of missing values
    'emp_title',                 # Too many unique values
    'zip_code',                  # Postal code, not needed
    'pymnt_plan',                # Only one unique value
    'purpose',                   # Dropped based on project requirements
]

test_data = test_data.drop(columns=columns_to_drop)

# Save the cleaned data
test_data.to_csv('/Users/yuguangsong/Documents/Datasets/loans_to_predict_cleaned.csv', index=False)

# Display shape and columns
print("Data cleaning completed:")
print(f"Test dataset shape: {test_data.shape}")

print("\nRemaining column names:")
print(test_data.columns.tolist())

# Display data type information
print("\nData type summary:")
print(test_data.dtypes.value_counts())

# Display missing values after cleaning
missing_test = test_data.isnull().sum()
missing_percent_test = (missing_test / len(test_data)) * 100
missing_stats = pd.DataFrame({
    'Missing Value Count': missing_test,
    'Missing Value Percentage': missing_percent_test
})

print("\nRemaining missing values:")
print(missing_stats[missing_stats['Missing Value Count'] > 0].sort_values('Missing Value Count', ascending=False))
</code></pre>

<hr>

<h2>Data Standardization (for <code>loans_to_predict_cleaned.csv</code>)</h2>
<p>
This code reads the cleaned prediction dataset and prepares it for modeling. It separates numeric and categorical columns, identifies date columns, and handles missing values. The <code>mths_since_last_delinq</code> column is imputed with <code>max_delinq + 1</code>, and rows with missing values in other numeric columns are dropped. The remaining numeric features are standardized using <code>StandardScaler</code> to ensure consistency in scale across features. The script also attempts to convert date columns into the number of days since a base date, enabling the model to use time-based information more effectively.
</p>

<pre><code class="language-python">
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# Read the cleaned data
test_data = pd.read_csv('/Users/yuguangsong/Documents/Datasets/loans_to_predict_cleaned.csv')

# Separate numerical and non-numerical columns, exclude the 'X' column
numeric_columns = test_data.select_dtypes(include=['int64', 'float64']).columns
numeric_columns = [col for col in numeric_columns if col != 'X']  # Exclude 'X' column
categorical_columns = test_data.select_dtypes(include=['object']).columns

print("Numeric variables:")
print(numeric_columns)
print("\nCategorical variables:")
print(categorical_columns.tolist())

# Identify date columns
date_columns = []
for column in test_data.columns:
    if column not in numeric_columns:
        try:
            pd.to_datetime(test_data[column].dropna().iloc[0])
            date_columns.append(column)
        except:
            continue

print("\nDate-type columns:")
print(date_columns)

# Check NaN counts for each numeric column
print("\nNaN counts for each numeric column:")
for column in numeric_columns:
    nan_count = test_data[column].isna().sum()
    if nan_count > 0:
        print(f"{column}: {nan_count} ({(nan_count/len(test_data)*100):.2f}%)")

# Special handling for mths_since_last_delinq - fill NaN with max value + 1
max_delinq = test_data['mths_since_last_delinq'].max()
test_data['mths_since_last_delinq'] = test_data['mths_since_last_delinq'].fillna(max_delinq + 1)

# Handle other NaN values by dropping rows with NaN in numeric columns (except for mths_since_last_delinq which is now handled)
test_data = test_data.dropna(subset=[col for col in numeric_columns if col != 'mths_since_last_delinq'])

print(f"\nDataset size after dropping NaNs:")
print(f"Test set: {test_data.shape}")

# Standardize the numeric variables (excluding 'X')
scaler = StandardScaler()
test_data[numeric_columns] = scaler.fit_transform(test_data[numeric_columns])

# Save the standardized data
test_data.to_csv('/Users/yuguangsong/Documents/Datasets/loans_to_predict_standardized.csv', index=False)

# Display summary statistics of standardized numeric variables
print("\nSummary statistics after standardization:")
print(test_data[numeric_columns].describe())

# Check for infinite or NaN values
print("\nCheck for infinite and NaN values:")
print("Number of infinite values in test set:", np.isinf(test_data[numeric_columns]).sum().sum())
print("Number of NaN values in test set:", np.isnan(test_data[numeric_columns]).sum().sum())

# Set a base date
base_date = pd.to_datetime('2000-01-01')

# Convert date columns to number of days since the base date
for date_column in date_columns:
    try:
        if date_column in test_data.columns:
            test_data[date_column] = pd.to_datetime(test_data[date_column], format='%b-%Y')
            test_data[f'{date_column}_days'] = (test_data[date_column] - base_date).dt.days
    except:
        print(f"Warning: Failed to convert date format in {date_column}")
        continue
</code></pre>

<p>After running these scripts, the resulting standardized dataset is free of NaNs and infinite values, with numeric features on a consistent scale. This prepared dataset is now ready to be fed into a predictive model.</p>

</body>
</html>
