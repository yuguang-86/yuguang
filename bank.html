<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Loan Prediction Project Report</title>
<style>
body {
    font-family: Arial, sans-serif;
    line-height: 1.6;
    margin: 20px;
    max-width: 1000px;
    color: #333;
}
h1, h2, h3, h4 {
    margin-bottom: 0.5em;
    color: #333;
}
hr {
    margin: 40px 0;
    border: none;
    border-top: 1px solid #ccc;
}
code, pre {
    font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
}
pre {
    background: #f5f5f5;
    border: 1px solid #ccc;
    padding: 10px;
    overflow: auto;
}
ul {
    margin: 0;
    padding-left: 20px;
}
</style>
</head>
<body>

<h1>Loan Prediction Project Report</h1>

<h2>Introduction</h2>
<p>
This project involved building a predictive model using historical loan data to estimate the probability of certain outcomes for new, unseen loan applications.
</p>

<p><strong>Data Sources:</strong></p>
<ul>
  <li><strong>Training Data:</strong> <code>loan_data.csv</code> – Historical loan information used for model training.</li>
  <li><strong>Prediction Data:</strong> <code>loans_to_predict.csv</code> – New loan applications for which we want to predict probabilities.</li>
</ul>

<p><strong>Workflow Steps:</strong></p>
<ol>
  <li>Initial Data Screening</li>
  <li>Data Cleaning</li>
  <li>Data Standardization</li>
  <li>Model Training</li>
  <li>Test Data Cleaning and Standardization</li>
  <li>Final Prediction</li>
</ol>

<p>Below, each step is explained in detail.</p>

<hr>

<h2>Step 1: Initial Data Screening</h2>
<p><strong>Objective:</strong> Assess the structure and quality of the training dataset before modeling.</p>

<p><strong>Actions:</strong></p>
<ul>
  <li>Checked the number of rows and columns.</li>
  <li>Summarized data types to identify numeric, categorical, and textual features.</li>
  <li>Evaluated missing values per column.</li>
  <li>Identified columns that may not be useful (e.g., unique IDs, high missing rates, low variance).</li>
</ul>

<p><strong>Outcome:</strong> This initial assessment guided decisions on which columns to keep, drop, or transform prior to modeling.</p>

<hr>

<h2>Step 2: Data Cleaning</h2>
<p>
A comprehensive data cleaning process was applied to both the training and prediction datasets. Below is a summary of the cleaning performed on <code>loans_to_predict.csv</code>. The same logic was consistently applied to the training data (<code>loan_data.csv</code>).
</p>

<p><strong>Key Actions for <code>loans_to_predict.csv</code>:</strong></p>
<ul>
  <li>Dropped columns with too many missing values or low predictive utility, such as <code>desc</code>, <code>mths_since_last_record</code>, <code>emp_title</code>, <code>zip_code</code>, <code>pymnt_plan</code>, and <code>purpose</code>.</li>
  <li>Ensured consistent handling of missing values.</li>
  <li>Removed unnecessary or one-value columns.</li>
</ul>

<p><strong>Results:</strong></p>
<ul>
  <li><strong>Dataset Shape:</strong> (133,723 rows, 29 columns) after dropping specified columns.</li>
  <li><strong>Data Types:</strong>
    <ul>
      <li><code>int64</code>: 14 columns</li>
      <li><code>float64</code>: 5 columns</li>
      <li><code>object</code>: 10 columns</li>
    </ul>
  </li>
  <li><strong>Remaining Missing Values:</strong>
    <ul>
      <li><code>mths_since_last_delinq</code>: ~48.23% missing</li>
      <li><code>revol_util</code>: ~0.046% missing</li>
      <li><code>title</code>: ~0.0217% missing</li>
    </ul>
  </li>
</ul>

<p>High-missing-value columns were considered for imputation strategies. Low-percentage missing values could be easily handled.</p>

<hr>

<h2>Step 3: Data Standardization</h2>
<p>
After cleaning, numerical features were standardized to have zero mean and unit variance. This is crucial for many model types sensitive to feature scales.
</p>

<p><strong>Key Steps:</strong></p>
<ul>
  <li><strong>Reading Cleaned Data:</strong> Loaded <code>loans_to_predict_cleaned.csv</code>.</li>
  <li><strong>Separating Numeric/Categorical Columns:</strong>
    <ul>
      <li>Numeric features were identified for standardization.</li>
      <li>Categorical features were kept separate for encoding.</li>
    </ul>
  </li>
  <li><strong>Date Columns:</strong> Attempted to convert date-like columns into a numeric representation (days since a base date).</li>
  <li><strong>Handling Missing Values in Numeric Columns:</strong>
    <ul>
      <li>Imputed <code>mths_since_last_delinq</code> with <code>max_delinq + 1</code>.</li>
      <li>Dropped rows with other numeric missing values after imputation.</li>
    </ul>
  </li>
  <li><strong>Standardization:</strong> Applied <code>StandardScaler</code> to numeric features.</li>
  <li><strong>Final Checks:</strong> Verified no infinite or NaN values remained post-standardization.</li>
</ul>

<p><strong>Outcome:</strong> The standardized dataset (<code>loans_to_predict_standardized.csv</code>) is now ready for model prediction with no missing or infinite values.</p>

<hr>

<h2>Step 4: Model Training and Evaluation</h2>

<p><strong>Training Steps:</strong></p>

<ol>
  <li><strong>Data Loading:</strong> Loaded the standardized training dataset <code>loans_data_standardized.csv</code>.</li>

  <li><strong>Preprocessing:</strong>
    <ul>
      <li>Split the data into features (X) and target (y), where <code>fully_paid</code> was the target.</li>
      <li>Handled missing values by imputing or dropping as needed.</li>
      <li>Processed categorical features via one-hot encoding.</li>
      <li>Converted date columns into numeric representations (e.g., year + month/12).</li>
      <li>Standardized all numeric features using <code>StandardScaler</code>.</li>
    </ul>
  </li>

  <li><strong>Data Splitting:</strong> Split into training (80%) and testing (20%) sets.</li>

  <li><strong>Class Imbalance Handling:</strong>
    <ul>
      <li>Identified imbalance: many more “fully paid” loans than “not fully paid.”</li>
      <li>Used SMOTE with a sampling strategy of 0.2 to improve minority class representation.</li>
    </ul>
  </li>
</ol>

<p><strong>Class Distribution:</strong></p>
<p>Before SMOTE:</p>
<ul>
  <li>Fully Paid (1): 213,283</li>
  <li>Not Fully Paid (0): 36,331</li>
</ul>

<p>After SMOTE:</p>
<ul>
  <li>Fully Paid (1): 213,283</li>
  <li>Not Fully Paid (0): 42,656</li>
</ul>

<p><strong>Model Training:</strong></p>
<ul>
  <li><strong>Chosen Model:</strong> Logistic Regression</li>
  <li><strong>Parameters:</strong> <code>max_iter=1000</code>, <code>class_weight='balanced'</code>, <code>C=0.1</code></li>
  <li>Trained on the SMOTE-balanced training data.</li>
</ul>

<p><strong>Model Evaluation:</strong></p>
<ul>
  <li>Predictions made on the test set.</li>
  <li><strong>Metrics:</strong>
    <ul>
      <li>Accuracy: 0.6536</li>
      <li>Precision: 0.9098</li>
      <li>Recall: 0.6611</li>
      <li>F1 Score: 0.7657</li>
      <li>AUC-ROC: 0.6871</li>
      <li>Average Precision: 0.9242</li>
    </ul>
  </li>
</ul>

<p><strong>Confusion Matrix:</strong></p>
<pre><code>
               Predicted 0    Predicted 1
  Actual 0        5453           3504
  Actual 1       18114          35333
</code></pre>

<p><strong>Interpretation:</strong></p>
<ul>
  <li>High precision (~91%) indicates that when the model predicts a loan as “fully paid,” it is usually correct.</li>
  <li>Recall (~66%) shows that out of all truly “fully paid” loans, the model identifies about two-thirds.</li>
  <li>The F1 score (~0.77) and AUC-ROC (~0.69) suggest moderate discriminative ability.</li>
  <li>High average precision (0.9242) indicates strong ranking performance.</li>
</ul>

<p><strong>Feature Importance:</strong></p>
<ul>
  <li>Top features included credit grade (e.g., <code>grade_B</code>, <code>grade_C</code>), loan amount variables, and some title-related features.</li>
  <li>These variables had the greatest influence on predictions.</li>
</ul>

<hr>

<h2>Step 5: Test Data Cleaning and Standardization</h2>
<p>
The same cleaning and standardization procedures were applied to <code>loans_to_predict.csv</code> to ensure consistency with the training set. All transformations were based on parameters derived from the training data to prevent data leakage.
</p>

<p><strong>Result:</strong> <code>loans_to_predict_standardized.csv</code> matches the training set’s preprocessing conditions, ready for final predictions.</p>

<hr>

<h2>Step 6: Final Prediction</h2>
<p><strong>Actions:</strong></p>
<ul>
  <li>Loaded the cleaned and standardized test dataset (<code>loans_to_predict_standardized.csv</code>).</li>
  <li>Applied the trained logistic regression model to generate probability predictions using <code>predict_proba()</code>.</li>
  <li>Saved these probabilities along with the corresponding loan IDs into <code>loan_predictions.csv</code>.</li>
</ul>

<p><strong>Sample Predictions:</strong></p>
<pre><code>
   X       pred
0  0   0.789654
1  1   0.543210
...
</code></pre>

<p>These probability scores help in making informed decisions regarding new loan applications.</p>

<hr>

<h2>Conclusion</h2>
<p>
This report detailed the full modeling pipeline:
</p>
<ul>
  <li><strong>Data Exploration and Cleaning:</strong> Ensured data integrity by removing irrelevant columns and handling missing values.</li>
  <li><strong>Standardization and Encoding:</strong> Achieved consistent scales for numeric features and encoded categorical variables properly.</li>
  <li><strong>Model Training with Class Imbalance Handling:</strong> Improved performance by addressing imbalance through SMOTE.</li>
  <li><strong>Evaluation:</strong> The logistic regression model displayed moderate performance, with strong precision but moderate recall and an acceptable AUC.</li>
  <li><strong>Final Prediction:</strong> Generated probability scores for new loan applications, enabling data-driven decision-making and risk assessment.</li>
</ul>

<p>
This end-to-end process, from data preparation through modeling and prediction, provides a reproducible and transparent approach. The final predictions serve as a valuable tool for loan decision strategies.
</p>

</body>
</html>
